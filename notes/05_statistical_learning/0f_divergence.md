##### f-divergence

> https://en.wikipedia.org/wiki/F-divergence

Suppose on measurable space $(\Omega, \F)$ there are two probability **measures** $P, Q$. And that $P \ll Q$.

By **Radon-Nikodym theorem**, there exists a unique $\dd P / \dd Q \in L(\Omega \to [0, \infty])$ derivative density.

Suppose $f: [0, \infty] \to (-\infty, +\infty]$ is a convex (smile) function. $f(0, \infty) \subset \R$. And $f(1) = 0$.

Define the $f$-divergence $D_f$ between distributions as following.
$$
D_f(P \Vert Q):= \int_\Omega f\p{\frac{\dd P}{ \dd Q}} \dd Q
$$

- By Jensen's inequality, all $f$-divergences are non-negative.
  $$
  D_f(P \Vert Q) = E_Q\s{f\p{\frac{\dd P}{\dd Q}}} \ge  f\p{E_Q \s{\frac{\dd P}{\dd Q}}} = f(1) = 0
  $$

Suppose $(\Omega, \F, \mu)$ is a **reference measure space**. And we have **density** $P = p \dd \mu$ and $Q = q \dd \mu$. We also write
$$
D_f(p \Vert q) := \int_{\Omega} f\p{\frac{p(x)}{q(x)}} q(x) \dd \mu(x)
$$

- The two definitions are equivalent.

Suppose $X, Y$ are two random variables to the same space $(\Omega, \F)$. We write $D_f(X \Vert Y) := D_f(P_X \Vert P_Y)$.

##### KL-divergence

The KL-divergence is a special $f$-divergence.

$f(x) = x\ln (x)$ gives the KL-divergence, and $g(x) = -\ln(x)$ gives the inverse KL-divergence.
$$
D_{\mathup{KL}}(p \Vert q) = \int p(x) \ln \frac{p(x)}{q(x)} \dd \mu(x) = - \int \ln \frac{q(x)}{p(x)} p(x) \dd \mu (x) = D_{-\mathup{KL}}(q \Vert p)
$$
Suppose $p_*(x)$ is a **data density** on $\Omega$, and $p_\theta(x)$ is a density generated by a statistical model.

- $\d{p_*(x)}{p_\theta(x)}$ is known as the **forward KL**.
  - Minimizing the forward KL is equivalent to maximizing log likelihood. Since
  $$
  \d{p_*}{p_\theta} = \int p_*(x) \log \frac{p_*(x)}{p_\theta(x)} \dd x = -H(p_*) - \int p_*(x) \log p_\theta(x) \dd x
  $$
- $\d{p_\theta(x)}{p_*(x)}$ is known as the **backward KL**.
  - Minimizing the backward KL is known to let $p_\theta$ focusing on a particular mode of $p$.
  - Suppose we know $p_*(x)$ is of the form $p_*(x) = e^{-U(x)} / Z$. Then for $X \sim p_\theta(x)$,
  $$
  \d{p_\theta}{p_*} = E\s{\log p_\theta(X)} + E [U(X)] + \log Z
  $$
    - Optimizing the backward KL can be done with unnormalized density $p_*$.
