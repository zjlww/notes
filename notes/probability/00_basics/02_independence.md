#### Independence

##### Notations: Independence and conditional independence

Events, set of events, and set of sets of events:
- We use plain $A \in \F$ to denote an event.
- We use font $\A \subseteq \F$ to denote a family of events.
- We use font $\mathscr A \subseteq \P (\F)$ to denote a collection of families of events.

The same three-tier hierarchy exists for random variables:
- We use plain $X \in \L(\Omega \to \Omega')$ to denote a random variable.
- We use font $\mathcal X$ for a family of r.v.s.
    - $\sigma(\X)$ is the generated $\sigma$-algebra by $\X$.
- We use font $\mathscr X$ for a collection of families of random variables.

##### Independence

Relative to a fixed probability space $(\Omega, \F, P)$. Then
- $\mathscr A$ is called **independent** if for $\A_1 ,\ldots, \A_n \in \mathscr A$, and any $A_k \in \mathcal A_k$, $P(\cap A_k) = \prod P(A_k)$.
    - Replace $\A \in \mathscr A$ in following ways does not alter independence of $\mathscr A$.
        - Replace $\A$ with $\{A^c : A \in \A\} \cup \A$.
        - Replace $\A$ with $\{+_k B_k: (B_k)_{k \in I \subseteq \N} \in \A\} \cup \A$.
        - Replace $\pi$-system $\A$ with $\sigma(\A)$ and vice versa.
- $\mathscr A$ is **independent** if and only if all subsets of $\mathscr A$ are independent.
- $\mathscr A$ is **independent**, $(\mathscr A_k)_{k \in K}$ is a partition of $\mathscr A$. Let $\mathcal B_k = \bigcup \mathscr A_k$ and $\mathscr B = \{\mathcal B_k\}_{k \in K}$. Then $\mathscr B$ is independent.

Concept of independence is extended to random variables:
- $\X = (X_i)_{i \in I}$ is **independent** if $\mathscr A = (\sigma(X_i))_{i \in I}$ is independent.
    - Suppose $X_i \in \L(\Omega \to \Omega_i, \F / \F_i)$. And $\E_i$ is a $\pi$-generator of $\F_i$.
        - Replace $X_i \in \X$ with $f \circ X_i$ where $f\in \L(\Omega_i \to \Omega_i')$ keeps $\X$ independent.
        - $X_i^{-1} \E_i$ is a $\pi$-generator of $\sigma(X_i)$.
        - $\X$ is independent iff $(X_i^{-1}\E_i)_{i \in I}$ are independent.
- $\X = (X_i)_{i\in I}$ is independent. Let $(\X_k)_{k \in K}$ be a partition of $\X$. Then $\mathscr A = (\sigma(\X_k))_{k \in K}$ is independent.
- Finite set of r.r.v.s. $\X = (X_i)_{i = 1}^n$ is **independent** if the joint distribution function $F_{\mathcal X}$ fully factorizes. That is $F_\mathcal X(x) = \prod_{i = 1}^n F_{X_i}(x_i)$.

##### Product space and independence

Suppose $(\Omega_i, \F_i, P_i)_{i = 1}^n$ are probability spaces. Let $(\Omega, \F, P) = (\times_i \Omega_i, \otimes_i \F_i, \times_i P_i)$. Then natural projections $(\pi_i: \Omega \to \Omega_i)_{i}$ are independent random variables in space $(\Omega, \F, P)$.

##### Expectation of independent r.v.s.

Suppose $(\Omega, \F, P)$ is a probability space, and $X, Y, Z \in \L^1(\Omega \to \R)$ are **independent** real random variables.

- Then $XY \in \L^1(\Omega \to \R)$, and $E[XY] = E[X] E[Y]$.
- $\cov(X, Y) = E[(X - EX)(Y - EY)] = EXY - EX EY$.
- $\var(X + Y) = \var(X) + \var(Y)$.

#### Product of Probability Spaces

##### (Binary) joint of real random variables

Suppose $(\Omega, \F, P)$ is a probability space. Suppose $X, Y :\Omega \to \R$ are r.r.v.s.
- Then $V = (X, Y)$ is a real vector valued random variable.

- $P_V = P_{X,Y}: \B[\R^2] \to [0, \infty]$ which is a measure on $(\R^2, \B[\R^2])$ is the **joint distribution** of $(X, Y)$.
    - For $S \subset \R$, we have $P_{X, Y}(S \times \R) = P_X(S)$.
    - $P_X(S)$ is called the **marginal distribution** of $X$.
    
- Define the joint distribution function $F_{X, Y}(x, y) = P_{X, Y}(X \le x, Y \le y)$.
    - The 2D Lebesgue-Stieltjes measure generated by $F_{X, Y}$ is exactly $P_{X, Y}$.
    - $F_X(x) = P_X(X \le x)$ is called the **marginal distribution function**.
    - $F_X(x) = \lim_{y \to \infty} F_{X, Y}(x, y) = F_{X, Y}(x, \infty)$.
    
- $f_V(x, y) = f_{X, Y}(x, y)$ is called the **joint density function** if $f_V: \R^2 \to [0, \infty]$ is Borel measurable and
    - it is normalized $\iint f_V(x,y) dx dy = 1$.
    - And $F_V(x, y) = \int_{-\infty}^x \int_{-\infty}^y f_V(x, y) dy dx$.
    
- With dominated convergence, we have: $$
    F_X(x) = \lim_{y \to \infty}F_V(x, y) = \int_{-\infty}^x \left(\int_{-\infty}^\infty f_V(x, y) dy\right) dx$$
    - Clearly $f_X(x) = \int f_V(x, y) dy$ is also a density function of $X$.
    - $f_X(x)$ obtained this way is Borel measurable.

    - $f_X(x)$ is called the **marginal density function**.

- **Independence and joint distribution function / joint density function**:
    - $X \perp Y \iff F_{X, Y}(x, y) = F_X(x) F_Y(y)$.
    - $X \perp Y \iff P_{X, Y} = P_X \times P_Y$.
    - $X \perp Y \iff f_V(x, y) = f_X(x)f_Y(y)$ if the joint densities exists.
    
- Vector expectation is defined as $E(X, Y) := (EX, EY)$.

- Suppose $\varphi(x, y): \R^2 \to \R$ is a Borel-measurable function. Then $\varphi(X, Y)$ is a random variable.
    - Then expectation of $\varphi(X, Y)$ obeys following formula:
      $$
      \begin{aligned}E[\varphi(X, Y)] &= \int_\Omega \varphi(X, Y) dP = \int_{\R^2} \varphi(x, y) dP_{X, Y}(x, y) = \int_{\R^2} \varphi(x, y) dF_{X, Y}\\ &= \int_{\R^2} \varphi(x, y) f_{X, Y}(x, y) dx dy\end{aligned}
      $$